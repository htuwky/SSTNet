import sys
import os
import glob
import numpy as np
import pandas as pd
from tqdm import tqdm

# [ä¿®æ”¹] ç§»é™¤ try/except å—ï¼Œç›´æ¥å¯¼å…¥ config
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import config

#ç¬¬äº”æ­¥  python check_data_stats.py
#ç¬¬äº”æ­¥æ˜¯ä¸ºäº†ç¡®å®šæ ‡å‡†/å½’ä¸€åŒ–æ•°æ®è¿›è¡Œçš„æ•°æ®åˆ†æï¼Œåæ¥è®­ç»ƒè€…å¯æ ¹æ®è‡ªå·±çš„æ•°æ®é›†è¿›è¡Œä¿®æ”¹
#è‹¥åªæ˜¯ç”¨æœ¬æ•°æ®é›†è¿›è¡Œä¿®è®­ç»ƒï¼Œç¬¬3ï¼Œ4ï¼Œ5æ­¥å‡å¯ä¸ç”¨è¿è¡Œ
# [ä¿®æ”¹] ä½¿ç”¨æ–°çš„ config å˜é‡ï¼Œåªåˆ†æè®­ç»ƒé›†æ•°æ®
TXT_DIR = config.TRAIN_TXT_DIR
NPY_PATH = config.CLIP_TRAIN_FEATURE_FILE


def analyze_physio_data(txt_dir):
    print(f"\nğŸ” [1/2] Scanning Physiological Data (.txt) in: {txt_dir}")

    files = glob.glob(os.path.join(txt_dir, '*.txt'))
    if len(files) == 0:
        print("âŒ No TXT files found!")
        return None

    # ä½¿ç”¨åˆ—è¡¨æš‚å­˜ (æ¯” np.append å¿«å¾—å¤š)
    all_x, all_y, all_dur, all_pupil = [], [], [], []
    valid_files = 0

    for f in tqdm(files, desc="Reading ALL TXT Files"):
        if os.path.getsize(f) == 0: continue
        try:
            # æ ¼å¼: Index, X, Y, Duration, Pupil
            df = pd.read_csv(f, header=None)
            if df.shape[1] < 5: continue

            # æ˜¾å¼è½¬æ¢ä¸º float32 èŠ‚çœå†…å­˜
            all_x.extend(df.iloc[:, 1].values.astype(np.float32))
            all_y.extend(df.iloc[:, 2].values.astype(np.float32))
            all_dur.extend(df.iloc[:, 3].values.astype(np.float32))
            all_pupil.extend(df.iloc[:, 4].values.astype(np.float32))
            valid_files += 1
        except Exception:
            continue

    print(f"âœ… Processed {valid_files} valid files.")

    # è½¬æ¢ä¸º Numpy æ•°ç»„
    stats = {
        "X-Coordinate": np.array(all_x),
        "Y-Coordinate": np.array(all_y),
        "Duration (ms)": np.array(all_dur),
        "Pupil Size": np.array(all_pupil)
    }

    results = {}
    print("-" * 75)
    print(f"{'Feature':<15} | {'Min':<10} | {'Max':<10} | {'Mean':<10} | {'Std':<10}")
    print("-" * 75)

    for name, data in stats.items():
        if len(data) == 0: continue
        _min, _max = np.min(data), np.max(data)
        _mean, _std = np.mean(data), np.std(data)
        print(f"{name:<15} | {_min:<10.4f} | {_max:<10.4f} | {_mean:<10.4f} | {_std:<10.4f}")
        results[name] = {'min': _min, 'max': _max, 'mean': _mean, 'std': _std}
    print("-" * 75)
    return results


def analyze_clip_features(npy_path):
    print(f"\nğŸ” [2/2] Analyzing ALL CLIP Features (.npy) from: {npy_path}")

    if not os.path.exists(npy_path):
        print("âŒ Feature file not found!")
        return None

    try:
        # åŠ è½½å­—å…¸
        data = np.load(npy_path, allow_pickle=True).item()
        total_seqs = len(data)
        print(f"âœ… Dictionary loaded. Containing {total_seqs} sequences.")

        print("â³ Stacking all features for rigorous analysis...")
        all_feats = []
        for k in tqdm(data.keys(), desc="Stacking"):

            item = data[k]
            # å…¼å®¹ v2.0 ç»“æ„ (å¦‚æœå®ƒæ˜¯å­—å…¸ï¼Œå°è¯•å †å  local å’Œ global ç‰¹å¾)
            if isinstance(item, dict):
                if 'local' in item:
                    all_feats.append(item['local'])
                # global æ˜¯ [1, 512]ï¼Œä¹Ÿå †å è¿›å»
                if 'global' in item:
                    all_feats.append(item['global'])
            elif isinstance(item, np.ndarray):
                all_feats.append(item)

        if not all_feats:
            print("âš ï¸ No valid features found in the NPY file.")
            return None

        # å †å æˆè¶…çº§å¤§çŸ©é˜µ: [Total_Points, 512]
        large_mat = np.vstack(all_feats)

        print(f"ğŸ“Š Global Matrix Shape: {large_mat.shape}")

        _min = np.min(large_mat)
        _max = np.max(large_mat)
        _mean = np.mean(large_mat)
        _std = np.std(large_mat)

        print("-" * 75)
        print(f"CLIP Feature Statistics (All {large_mat.shape[0]} points)")
        print(f"Min:  {_min:.6f}")
        print(f"Max:  {_max:.6f}")
        print(f"Mean: {_mean:.6f} (Should be close to 0)")
        print(f"Std:  {_std:.6f}")
        print("-" * 75)

        return {'min': _min, 'max': _max, 'mean': _mean, 'std': _std}

    except Exception as e:
        print(f"âŒ Error reading/processing .npy: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    # 1. ç»Ÿè®¡ç”Ÿç†æ•°æ®
    physio_stats = analyze_physio_data(TXT_DIR)

    # 2. ç»Ÿè®¡ CLIP æ•°æ®
    clip_stats = analyze_clip_features(NPY_PATH)

    # 3. ç»™å‡ºä¸¥è°¨å»ºè®®
    if physio_stats and clip_stats:
        print("\nğŸ’¡ [Rigorous Normalization Strategy]")
        print("=" * 60)

        # æ£€æŸ¥ CLIP åˆ†å¸ƒ
        print(f"1. CLIP Visual Embeddings:")
        print(f"   Range: [{clip_stats['min']:.4f}, {clip_stats['max']:.4f}]")
        print(f"   Dist : Mean~{clip_stats['mean']:.2f}, Std~{clip_stats['std']:.2f}")

        if abs(clip_stats['mean']) < 0.1 and 0.8 < clip_stats['std'] < 1.2:
            target_norm_desc = "Standardization (Mean=0, Std=1)"
            method = "z-score"
        else:
            target_norm_desc = "Min-Max Scaling (-1 to 1)"
            method = "minmax"

        print(f"   -> Target Distribution: {target_norm_desc}")

        print(f"\n2. Physiological Data Normalization Parameters (Copy to config.py):")
        print(f"   (Using 3-Sigma rule to handle outliers for robust scaling)")

        for name, stat in physio_stats.items():
            # ä½¿ç”¨ 3-Sigma ç¡®å®šé²æ£’çš„è¾¹ç•Œï¼Œé˜²æ­¢æå€¼ç ´åå½’ä¸€åŒ–
            robust_max = stat['mean'] + 3 * stat['std']
            robust_min = max(0, stat['mean'] - 3 * stat['std'])  # ç‰©ç†é‡é€šå¸¸éè´Ÿ

            # å¦‚æœæå€¼æ²¡æœ‰åç¦»å¤ªè¿œï¼Œå°±ç”¨çœŸå®æå€¼
            final_max = min(stat['max'], robust_max) if stat['max'] > robust_max * 1.5 else stat['max']
            final_min = stat['min']  # æœ€å°å€¼é€šå¸¸æ¯”è¾ƒç¨³å®š

            print(f"   ğŸ”¹ {name}:")
            print(f"      CONFIG_MIN = {final_min:.4f}")
            print(f"      CONFIG_MAX = {final_max:.4f}")
            if stat['max'] > robust_max:
                print(
                    f"      (Note: Original Max was {stat['max']:.4f}, clipped to {final_max:.4f} to exclude outliers)")