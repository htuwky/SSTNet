import sys
import os
import argparse  # [æ–°å¢] å¯¼å…¥ argparse

# å°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import config  #
import glob
import torch
import clip
from PIL import Image
import numpy as np
import pandas as pd
from tqdm import tqdm

#2ç¬¬äºŒæ­¥ï¼Œpython data_process/generate_clip_features.py --test
# python data_process/generate_clip_features.py --train
def safe_crop(image, x, y, crop_size=224):
    """æ ¹æ®æ³¨è§†ç‚¹åæ ‡å®‰å…¨è£å‰ªå›¾åƒå—ï¼ˆåŒ…å«è¾¹ç•Œå¡«å……é€»è¾‘ï¼‰"""
    w, h = image.size
    half = crop_size // 2
    left, top, right, bottom = x - half, y - half, x + half, y + half

    if left >= 0 and top >= 0 and right <= w and bottom <= h:
        return image.crop((left, top, right, bottom))

    pad_img = Image.new("RGB", (crop_size, crop_size), (0, 0, 0))
    src_left, src_top = max(0, left), max(0, top)
    src_right, src_bottom = min(w, right), min(h, bottom)

    if src_right > src_left and src_bottom > src_top:
        crop_part = image.crop((src_left, src_top, src_right, src_bottom))
        pad_img.paste(crop_part, (max(0, -left), max(0, -top)))
    return pad_img


def build_image_map(root_dir):
    """é€’å½’æ‰«ææ–‡ä»¶å¤¹ï¼Œå»ºç«‹ {å›¾ç‰‡å: å®Œæ•´è·¯å¾„} çš„æ˜ å°„è¡¨"""
    print(f"ğŸ” Scanning image directory: {root_dir} ...")
    image_map = {}
    count = 0
    for root, dirs, files in os.walk(root_dir):
        for f in files:
            if f.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp')):
                # ä½¿ç”¨ os.path.splitext ç¡®ä¿åªå»é™¤æœ€åä¸€ä¸ªæ‰©å±•å
                name_no_ext = os.path.splitext(f)[0]
                full_path = os.path.join(root, f)
                image_map[name_no_ext] = full_path
                count += 1
    print(f"âœ… Indexed {count} images.")
    return image_map


def main():
    # --- 1. å‚æ•°è§£æä¸è·¯å¾„ç¡®å®š ---
    parser = argparse.ArgumentParser(description="Generate CLIP features for train or test set.")
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument('--train', action='store_true', help='Extract features for the Training/Validation set.')
    group.add_argument('--test', action='store_true', help='Extract features for the Test set.')
    args = parser.parse_args()

    if args.train:
        txt_dir = config.TRAIN_TXT_DIR  #
        output_path = config.CLIP_TRAIN_FEATURE_FILE  #
        mode_name = "Training/Validation"
    elif args.test:
        txt_dir = config.TEST_TXT_DIR  #
        output_path = config.CLIP_TEST_FEATURE_FILE  #
        mode_name = "Testing"
    else:
        # should not happen
        return

    device = config.DEVICE if torch.cuda.is_available() else "cpu"  #
    img_dir = config.IMAGE_DIR  #

    print(f"ğŸš€ Starting CLIP Feature Extraction for {mode_name} set.")
    print(f"   Reading TXT files from: {txt_dir}")
    print(f"   Output NPY to: {output_path}")

    # --- 2. åˆå§‹åŒ–æ¨¡å‹ ---
    print(f"ğŸ”„ Loading CLIP ({config.CLIP_MODEL_NAME}) on {device}...")  #
    model, preprocess = clip.load(config.CLIP_MODEL_NAME, device=device)  #
    model.eval()

    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    # 3. å»ºç«‹å›¾ç‰‡åœ°å›¾
    image_path_map = build_image_map(img_dir)

    txt_files = glob.glob(os.path.join(txt_dir, '*.txt'))
    print(f"ğŸ“‚ Found {len(txt_files)} sequence files.")

    feature_dict = {}
    error_count = 0
    skip_count = 0

    # --- 4. æå–å¾ªç¯ï¼ˆåŒæµé€»è¾‘å®Œæ•´ä¿ç•™ï¼‰ ---
    for txt_path in tqdm(txt_files, desc="Extracting Local+Global"):
        filename_key = os.path.basename(txt_path).split('.txt')[0]

        try:
            # è§£ææ–‡ä»¶å
            try:
                # [æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨æ¡ä»¶é€»è¾‘åŒºåˆ† Train/Test è§£æ]
                if args.train:
                    # è®­ç»ƒ/éªŒè¯é›†: Subject ID é€šå¸¸æ²¡æœ‰ä¸‹åˆ’çº¿ (e.g., T001_A01)
                    subject_id, image_name_str = filename_key.split('_', 1)
                elif args.test:
                    # æµ‹è¯•é›†: Test_XXX_ImageName (ImageName å¯èƒ½å«ä¸‹åˆ’çº¿ï¼Œä¾‹å¦‚ cat_012)

                    # 1. å°†æ–‡ä»¶åé€šè¿‡æ‰€æœ‰ä¸‹åˆ’çº¿å®Œå…¨åˆ†å‰²
                    parts = filename_key.split('_')

                    # 2. æ£€æŸ¥é•¿åº¦æ˜¯å¦æ»¡è¶³ Test_XXX_... çš„æ ¼å¼
                    if len(parts) >= 3:
                        # Subject ID æ€»æ˜¯å‰ä¸¤éƒ¨åˆ† (Test_000)
                        subject_id = f"{parts[0]}_{parts[1]}"

                        # å›¾ç‰‡åæ˜¯ç¬¬ä¸‰éƒ¨åˆ†åˆ°æœ«å°¾çš„æ‰€æœ‰å†…å®¹ï¼Œé‡æ–°ä»¥ä¸‹åˆ’çº¿è¿æ¥èµ·æ¥
                        image_name_str = '_'.join(parts[2:])
                    else:
                        # å¦‚æœä¸æ»¡è¶³ Test_XXX_... çš„æ ¼å¼ï¼Œè·³è¿‡
                        skip_count += 1
                        continue

            except ValueError:
                # å¦‚æœæ–‡ä»¶åä¸­è¿ä¸€ä¸ªä¸‹åˆ’çº¿éƒ½æ²¡æœ‰ï¼Œåˆ™è·³è¿‡
                skip_count += 1;
                continue

            # 2. ä»åœ°å›¾é‡ŒæŸ¥æ‰¾å›¾ç‰‡è·¯å¾„
            if image_name_str in image_path_map:
                img_full_path = image_path_map[image_name_str]
            else:
                # æ‰¾ä¸åˆ°å›¾ç‰‡ï¼Œè·³è¿‡
                skip_count += 1;
                continue

            # è¯»å–æ•°æ®
            if os.path.getsize(txt_path) == 0: continue
            df = pd.read_csv(txt_path, header=None)
            coords = df.iloc[:, 1:3].values

            img = Image.open(img_full_path).convert("RGB")

            # === æå–å…¨å±€ç‰¹å¾ (Global) ===
            global_tensor = preprocess(img).unsqueeze(0).to(device)
            with torch.no_grad():
                global_feat = model.encode_image(global_tensor).cpu().numpy()
                # CLIP é»˜è®¤å½’ä¸€åŒ–
                global_feat = global_feat / np.linalg.norm(global_feat, axis=1, keepdims=True)

            # === æå–å±€éƒ¨ç‰¹å¾ (Local) ===
            patches = []
            for (x, y) in coords:
                patch = safe_crop(img, int(x), int(y), config.CROP_SIZE)  #
                patches.append(preprocess(patch))

            if not patches: continue

            input_tensor = torch.stack(patches).to(device)
            local_feats_list = []
            with torch.no_grad():
                # æ‰¹é‡æå–å±€éƒ¨ç‰¹å¾
                for i in range(0, len(input_tensor), config.EXTRACT_BATCH_SIZE):  #
                    batch = input_tensor[i: i + config.EXTRACT_BATCH_SIZE]
                    feat = model.encode_image(batch)
                    local_feats_list.append(feat.cpu().numpy())

            local_feats = np.concatenate(local_feats_list, axis=0).astype(np.float32)
            # CLIP é»˜è®¤å½’ä¸€åŒ–
            local_feats = local_feats / np.linalg.norm(local_feats, axis=1, keepdims=True)

            # === å­˜å…¥å­—å…¸ ===
            feature_dict[filename_key] = {
                'local': local_feats,
                'global': global_feat
            }

        except Exception as e:
            # print(f"Error: {e}")
            error_count += 1;
            continue

    print(f"ğŸ’¾ Saving features to {output_path}...")
    np.save(output_path, feature_dict)
    print(f"âœ… Done! Saved {len(feature_dict)} sequences. (Skipped: {skip_count}, Errors: {error_count})")


if __name__ == "__main__":
    main()