import sys
import os

# å°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥è·¯å¾„ï¼Œç¡®ä¿èƒ½ import config
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import config  # å¯¼å…¥é…ç½®
import glob
import torch
import clip
from PIL import Image
import numpy as np
import pandas as pd
from tqdm import tqdm


def safe_crop(image, x, y, crop_size=224):
    """
    ä»¥ (x, y) ä¸ºä¸­å¿ƒè£å‰ªå›¾ç‰‡ã€‚è‡ªåŠ¨å¤„ç†è¶Šç•Œæƒ…å†µ (Padding é»‘è‰²èƒŒæ™¯)ã€‚
    """
    w, h = image.size
    half = crop_size // 2

    left = x - half
    top = y - half
    right = x + half
    bottom = y + half

    # åœºæ™¯A: å®Œå…¨åœ¨å›¾å†…
    if left >= 0 and top >= 0 and right <= w and bottom <= h:
        return image.crop((left, top, right, bottom))

    # åœºæ™¯B: è¶Šç•Œï¼Œéœ€è¦ Padding
    pad_img = Image.new("RGB", (crop_size, crop_size), (0, 0, 0))

    src_left = max(0, left)
    src_top = max(0, top)
    src_right = min(w, right)
    src_bottom = min(h, bottom)

    if src_right > src_left and src_bottom > src_top:
        crop_part = image.crop((src_left, src_top, src_right, src_bottom))
        dst_left = max(0, -left)
        dst_top = max(0, -top)
        pad_img.paste(crop_part, (dst_left, dst_top))

    return pad_img


def build_image_map(root_dir):
    """
    é€’å½’æ‰«ææ–‡ä»¶å¤¹ï¼Œå»ºç«‹ {å›¾ç‰‡å(æ— åç¼€): å®Œæ•´è·¯å¾„} çš„æ˜ å°„è¡¨
    """
    print(f"ğŸ” Scanning image directory: {root_dir} ...")
    image_map = {}
    count = 0
    for root, dirs, files in os.walk(root_dir):
        for f in files:
            if f.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp')):
                # è·å–ä¸å¸¦åç¼€çš„æ–‡ä»¶åï¼Œä½œä¸ºKey
                name_no_ext = os.path.splitext(f)[0]
                full_path = os.path.join(root, f)
                image_map[name_no_ext] = full_path
                count += 1
    print(f"âœ… Indexed {count} images.")
    return image_map


def main():
    # 1. å‡†å¤‡é…ç½®
    device = config.DEVICE if torch.cuda.is_available() else "cpu"
    print(f"ğŸ”„ Loading CLIP ({config.CLIP_MODEL_NAME}) on {device}...")
    model, preprocess = clip.load(config.CLIP_MODEL_NAME, device=device)
    model.eval()

    txt_dir = config.TXT_DIR
    img_dir = config.IMAGE_DIR
    output_path = config.CLIP_FEATURE_FILE

    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    # 2. [æ ¸å¿ƒæ”¹è¿›] å»ºç«‹å›¾ç‰‡è·¯å¾„æ˜ å°„è¡¨
    # ä¸ç®¡å›¾ç‰‡è—åœ¨å“ªä¸ªå­æ–‡ä»¶å¤¹ï¼Œåªè¦åå­—å¯¹å¾—ä¸Šï¼Œå°±èƒ½æ‰¾åˆ°
    image_path_map = build_image_map(img_dir)

    # è·å–æ‰€æœ‰ TXT æ–‡ä»¶
    txt_files = glob.glob(os.path.join(txt_dir, '*.txt'))
    print(f"ğŸ“‚ Found {len(txt_files)} sequence files.")

    feature_dict = {}
    error_count = 0
    skip_count = 0

    # 3. ä¸»å¾ªç¯
    for txt_path in tqdm(txt_files, desc="Extracting Features"):
        filename_key = os.path.basename(txt_path).split('.txt')[0]

        # --- A. è§£ææ–‡ä»¶å ---
        # è§„åˆ™ï¼šID_ImageName (ä¾‹å¦‚ 002_act_001)
        try:
            subject_id, image_name_str = filename_key.split('_', 1)
        except ValueError:
            print(f"âš ï¸ Skipping invalid filename: {filename_key}")
            skip_count += 1
            continue

        # --- B. [æ”¹è¿›] æŸ¥æ‰¾å›¾ç‰‡æ–‡ä»¶ ---
        # ç›´æ¥ä»åœ°å›¾é‡ŒæŸ¥ï¼Œä¸å†æ‹¼æ¥è·¯å¾„çŒœ
        if image_name_str in image_path_map:
            img_full_path = image_path_map[image_name_str]
        else:
            # print(f"âš ï¸ Image not found in map: {image_name_str}")
            skip_count += 1
            continue

        try:
            # --- C. è¯»å–çœ¼åŠ¨æ•°æ® ---
            if os.path.getsize(txt_path) == 0: continue
            df = pd.read_csv(txt_path, header=None)
            coords = df.iloc[:, 1:3].values  # [[x, y], ...]

            # --- D. è£å‰ªä¸æå– ---
            img = Image.open(img_full_path).convert("RGB")

            patches = []
            for (x, y) in coords:
                patch = safe_crop(img, int(x), int(y), config.CROP_SIZE)
                patches.append(preprocess(patch))

            if not patches: continue

            # å †å å¹¶é€å…¥ GPU
            input_tensor = torch.stack(patches).to(device)

            # åˆ†æ‰¹æå–
            features_list = []
            with torch.no_grad():
                for i in range(0, len(input_tensor), config.EXTRACT_BATCH_SIZE):
                    batch = input_tensor[i: i + config.EXTRACT_BATCH_SIZE]
                    feat = model.encode_image(batch)
                    features_list.append(feat.cpu().numpy())

            final_features = np.concatenate(features_list, axis=0).astype(np.float32)
            feature_dict[filename_key] = final_features

        except Exception as e:
            print(f"âŒ Error processing {filename_key}: {e}")
            error_count += 1
            continue

    # 4. ä¿å­˜
    print(f"ğŸ’¾ Saving features to {output_path}...")
    np.save(output_path, feature_dict)
    print(f"âœ… All Done! Saved {len(feature_dict)} sequences. (Skipped: {skip_count}, Errors: {error_count})")


if __name__ == "__main__":
    main()