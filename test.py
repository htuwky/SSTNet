import os
import torch
import numpy as np
import pandas as pd
import argparse
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader
import sys

# å¯¼å…¥é¡¹ç›®æ¨¡å—
import config
from models.sstnet import SSTNet


# ==========================================
# 1. å®šä¹‰æµ‹è¯•ä¸“ç”¨çš„ Dataset
# ==========================================
class TestDataset(Dataset):
    def __init__(self):
        self.max_len = config.MAX_SEQ_LEN

        # 1. åŠ è½½æµ‹è¯•é›†è§†è§‰ç‰¹å¾
        print(f"ğŸ”„ Loading TEST visual features from {config.CLIP_TEST_FEATURE_FILE} ...")
        if not os.path.exists(config.CLIP_TEST_FEATURE_FILE):
            raise FileNotFoundError(
                "âŒ Test feature file not found! Please run 'generate_clip_features.py --test' first.")

        self.visual_data = np.load(config.CLIP_TEST_FEATURE_FILE, allow_pickle=True).item()

        # è·å–æ‰€æœ‰æ ·æœ¬çš„ key (ä¾‹å¦‚: Test_001_cat)
        self.keys = list(self.visual_data.keys())
        print(f"âœ… Test set loaded: {len(self.keys)} sequences found.")

    def __getitem__(self, idx):
        key = self.keys[idx]

        # --- A. è·å–è§†è§‰ç‰¹å¾ ---
        data_pack = self.visual_data[key]
        visual_local = data_pack['local']  # [Seq, 512]
        visual_global = data_pack['global']  # [1, 512]

        # --- B. è·å–ç”Ÿç†ç‰¹å¾ ---
        # æ„é€ å¯¹åº”çš„ txt è·¯å¾„
        txt_path = os.path.join(config.TEST_TXT_DIR, f"{key}.txt")

        physio_feat = np.zeros((visual_local.shape[0], config.PHYSIO_DIM), dtype=np.float32)

        if os.path.exists(txt_path) and os.path.getsize(txt_path) > 0:
            try:
                # è¯»å– txt (æ ¼å¼: Index, X, Y, Duration, Pupil)
                df = pd.read_csv(txt_path, header=None)
                raw_data = df.iloc[:, 1:3].values.astype(np.float32)

                # å½’ä¸€åŒ– (å¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´!)
                min_vec = np.array([config.SCREEN_X_MIN, config.SCREEN_Y_MIN], dtype=np.float32)
                max_vec = np.array([config.SCREEN_X_MAX, config.SCREEN_Y_MAX], dtype=np.float32)

                raw_data = np.clip(raw_data, min_vec, max_vec)
                norm_0_1 = (raw_data - min_vec) / (max_vec - min_vec + 1e-6)
                physio_feat = norm_0_1 * 2 - 1
            except Exception:
                pass

        # --- C. Padding & Mask ---
        seq_len = visual_local.shape[0]
        target_len = self.max_len

        padded_local = np.zeros((target_len, config.INPUT_DIM), dtype=np.float32)
        padded_physio = np.zeros((target_len, config.PHYSIO_DIM), dtype=np.float32)
        mask = np.zeros(target_len, dtype=np.float32)

        valid_len = min(seq_len, target_len)

        padded_local[:valid_len] = visual_local[:valid_len]

        fill_len = min(valid_len, physio_feat.shape[0])
        padded_physio[:fill_len] = physio_feat[:fill_len]
        mask[:fill_len] = 1.0

        # è§£æ Subject ID (å‡è®¾æ–‡ä»¶åæ ¼å¼ä¸º Test_001_ImgName)
        # å¦‚æœä½ çš„æµ‹è¯•é›†å‘½åä¸åŒï¼Œè¿™é‡Œéœ€è¦è°ƒæ•´
        parts = key.split('_')
        if len(parts) >= 2:
            # å–å‰ä¸¤ä¸ªéƒ¨åˆ†ä½œä¸º IDï¼Œä¾‹å¦‚ "Test_001"
            subject_id = f"{parts[0]}_{parts[1]}"
        else:
            subject_id = key

        return (
            torch.FloatTensor(padded_local),
            torch.FloatTensor(visual_global),
            torch.FloatTensor(padded_physio),
            torch.FloatTensor(mask),
            subject_id,
            key  # è¿”å›å®Œæ•´æ–‡ä»¶åä»¥ä¾¿è®°å½•
        )

    def __len__(self):
        return len(self.keys)


# ==========================================
# 2. ä¸»æµ‹è¯•æµç¨‹
# ==========================================
def main():
    parser = argparse.ArgumentParser(description="SSTNet Inference/Test Script")
    parser.add_argument('--ckpt', type=str, required=True, help='Path to the model checkpoint (.pth)')
    parser.add_argument('--output', type=str, default='test_results.csv', help='Path to save results CSV')
    args = parser.parse_args()

    device = torch.device(config.DEVICE if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ Starting Inference using: {device}")

    # 1. åŠ è½½æ¨¡å‹
    print(f"ğŸ”„ Loading Model from: {args.ckpt}")
    model = SSTNet().to(device)

    # åŠ è½½æƒé‡ (å¤„ç†å¯èƒ½å­˜åœ¨çš„ 'model' é”®)
    checkpoint = torch.load(args.ckpt, map_location=device)
    if 'model' in checkpoint:
        state_dict = checkpoint['model']
    else:
        state_dict = checkpoint

    model.load_state_dict(state_dict)
    model.eval()

    # 2. å‡†å¤‡æ•°æ®
    test_dataset = TestDataset()
    test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=0)

    # 3. æ¨ç†å¾ªç¯
    results_list = []

    # ç”¨äºç—…äººçº§èšåˆ
    patient_scores = {}

    print("running inference...")
    with torch.no_grad():
        for local_vis, global_vis, physio, mask, subject_ids, keys in tqdm(test_loader):
            local_vis = local_vis.to(device)
            global_vis = global_vis.to(device).squeeze(1)  # [B, 1, 512] -> [B, 512]
            physio = physio.to(device)
            mask = mask.to(device)

            # å‰å‘ä¼ æ’­
            logits = model(local_vis, global_vis, physio, mask)

            # è½¬æ¦‚ç‡ (Sigmoid)
            probs = torch.sigmoid(logits).cpu().numpy().flatten()

            # è®°å½•ç»“æœ
            for i in range(len(keys)):
                subj_id = subject_ids[i]
                prob = probs[i]
                fname = keys[i]

                # å­˜å…¥åˆ—è¡¨
                results_list.append({
                    "Subject_ID": subj_id,
                    "File_Name": fname,
                    "Probability": prob
                })

                # èšåˆé€»è¾‘
                if subj_id not in patient_scores:
                    patient_scores[subj_id] = []
                patient_scores[subj_id].append(prob)

    # 4. è®¡ç®—ç—…äººçº§æœ€ç»ˆç»“æœ (Mean Voting)
    print("\nğŸ“Š Aggregating Patient-Level Results...")
    final_patient_results = []

    for subj_id, scores in patient_scores.items():
        avg_score = np.mean(scores)
        # å‡è®¾é˜ˆå€¼ 0.5ï¼Œæ ¹æ®éœ€è¦è°ƒæ•´
        pred_label = 1 if avg_score > 0.5 else 0

        final_patient_results.append({
            "Subject_ID": subj_id,
            "Avg_Probability": avg_score,
            "Prediction": pred_label,
            "Sequence_Count": len(scores)
        })

    # 5. ä¿å­˜æ–‡ä»¶
    # ä¿å­˜è¯¦ç»†çš„æ¯å¼ å›¾çš„ç»“æœ
    df_detail = pd.DataFrame(results_list)
    detail_csv = args.output.replace('.csv', '_detail.csv')
    df_detail.to_csv(detail_csv, index=False)

    # ä¿å­˜ç—…äººçº§æœ€ç»ˆè¯Šæ–­
    df_patient = pd.DataFrame(final_patient_results)
    df_patient.to_csv(args.output, index=False)

    print(f"âœ… Done!")
    print(f"   - Detailed results: {detail_csv}")
    print(f"   - Patient diagnosis: {args.output}")


if __name__ == "__main__":
    main()