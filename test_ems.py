import os
import torch
import numpy as np
import pandas as pd
import argparse
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader
import sys

# å¯¼å…¥é¡¹ç›®é…ç½®å’Œæ¨¡å‹
import config
from models.sstnet import SSTNet


# ==========================================
# 1. å®šä¹‰æµ‹è¯•ä¸“ç”¨ Dataset
# ==========================================
class EMSTestDataset(Dataset):
    def __init__(self):
        self.max_len = config.MAX_SEQ_LEN
        self.feature_path = config.CLIP_TEST_FEATURE_FILE
        self.txt_root = config.TEST_TXT_DIR

        print(f"ğŸ”„ Loading EMS Test features from: {self.feature_path}")
        if not os.path.exists(self.feature_path):
            raise FileNotFoundError(
                f"âŒ Feature file not found: {self.feature_path}\nPlease run 'python data_process/generate_clip_features.py --test' first.")

        self.visual_data = np.load(self.feature_path, allow_pickle=True).item()

        # è·å–æ‰€æœ‰åºåˆ— Key
        self.keys = list(self.visual_data.keys())
        print(f"âœ… Loaded {len(self.keys)} sequences.")

        # ç®€å•ç»Ÿè®¡ä¸€ä¸‹åŒ…å«å¤šå°‘ä¸ªå—è¯•è€…
        subjects = set()
        for k in self.keys:
            # å‡è®¾æ–‡ä»¶åæ ¼å¼: Test_001_ImageName...
            parts = k.split('_')
            if len(parts) >= 2:
                subjects.add(f"{parts[0]}_{parts[1]}")
        print(f"ğŸ‘¥ Unique Subjects identified: {len(subjects)} (Target is ~48)")

    def __getitem__(self, idx):
        key = self.keys[idx]

        # --- A. è·å–è§†è§‰ç‰¹å¾ ---
        data_pack = self.visual_data[key]
        visual_local = data_pack['local']  # [Seq, 512]
        visual_global = data_pack['global']  # [1, 512] æˆ–è€… [512]

        # --- B. è·å–ç”Ÿç†ç‰¹å¾ ---
        # æ„é€  txt è·¯å¾„
        txt_path = os.path.join(self.txt_root, f"{key}.txt")

        physio_feat = np.zeros((visual_local.shape[0], config.PHYSIO_DIM), dtype=np.float32)

        # å°è¯•è¯»å–å¹¶å½’ä¸€åŒ–ç”Ÿç†æ•°æ®
        if os.path.exists(txt_path) and os.path.getsize(txt_path) > 0:
            try:
                # è¯»å– txt (Index, X, Y, Duration, Pupil)
                df = pd.read_csv(txt_path, header=None)
                raw_data = df.iloc[:, 1:5].values.astype(np.float32)

                # å½’ä¸€åŒ– (ä½¿ç”¨ config ä¸­çš„å‚æ•°)
                min_vec = np.array([config.SCREEN_X_MIN, config.SCREEN_Y_MIN, config.DUR_MIN, config.PUPIL_MIN],
                                   dtype=np.float32)
                max_vec = np.array([config.SCREEN_X_MAX, config.SCREEN_Y_MAX, config.DUR_MAX, config.PUPIL_MAX],
                                   dtype=np.float32)

                raw_data = np.clip(raw_data, min_vec, max_vec)
                norm_0_1 = (raw_data - min_vec) / (max_vec - min_vec + 1e-6)
                physio_feat = norm_0_1 * 2 - 1  # æ˜ å°„åˆ° [-1, 1]
            except Exception:
                pass  # å‡ºé”™ä¿æŒå…¨0

        # --- C. Padding & Mask ---
        seq_len = visual_local.shape[0]
        target_len = self.max_len

        padded_local = np.zeros((target_len, config.INPUT_DIM), dtype=np.float32)
        padded_physio = np.zeros((target_len, config.PHYSIO_DIM), dtype=np.float32)
        mask = np.zeros(target_len, dtype=np.float32)

        valid_len = min(seq_len, target_len)

        padded_local[:valid_len] = visual_local[:valid_len]

        fill_len = min(valid_len, physio_feat.shape[0])
        padded_physio[:fill_len] = physio_feat[:fill_len]
        mask[:fill_len] = 1.0

        # è§£æ Subject ID (æ ¼å¼: Test_001)
        parts = key.split('_')
        if len(parts) >= 2:
            subject_id = f"{parts[0]}_{parts[1]}"
        else:
            subject_id = key

        return (
            torch.FloatTensor(padded_local),
            torch.FloatTensor(visual_global),
            torch.FloatTensor(padded_physio),
            torch.FloatTensor(mask),
            subject_id
        )

    def __len__(self):
        return len(self.keys)


# ==========================================
# 2. ä¸»ç¨‹åº
# ==========================================
def main():
    parser = argparse.ArgumentParser(description="Generate prob_test.txt for EMS dataset")
    parser.add_argument('--ckpt', type=str, required=True, help='Path to model checkpoint (.pth)')
    parser.add_argument('--threshold', type=float, default=0.5, help='Threshold for binarization')
    args = parser.parse_args()

    # è¾“å‡ºæ–‡ä»¶è·¯å¾„
    output_file = 'prob_test.txt'

    device = torch.device(config.DEVICE if torch.cuda.is_available() else "cpu")

    # 1. åŠ è½½æ¨¡å‹
    print(f"\nğŸš€ Loading Model from: {args.ckpt}")
    model = SSTNet().to(device)

    checkpoint = torch.load(args.ckpt, map_location=device)
    # å…¼å®¹ä¸åŒçš„ä¿å­˜æ ¼å¼
    if 'model' in checkpoint:
        model.load_state_dict(checkpoint['model'])
    else:
        model.load_state_dict(checkpoint)

    model.eval()

    # 2. å‡†å¤‡æ•°æ®
    test_dataset = EMSTestDataset()
    # Batch Size è®¾å¤§ä¸€ç‚¹å¯ä»¥åŠ å¿«æ¨ç†é€Ÿåº¦
    test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=0)

    # 3. æ¨ç†ä¸èšåˆ
    patient_scores = {}  # { 'Test_001': [0.1, 0.2, ...], ... }

    print(f"ğŸ”® Running Inference on {device}...")

    with torch.no_grad():
        for local_vis, global_vis, physio, mask, subject_ids in tqdm(test_loader):
            local_vis = local_vis.to(device)
            # å¤„ç† global ç»´åº¦: [B, 1, 512] -> [B, 512]
            if global_vis.dim() == 3:
                global_vis = global_vis.squeeze(1)
            global_vis = global_vis.to(device)

            physio = physio.to(device)
            mask = mask.to(device)

            # å‰å‘ä¼ æ’­
            logits = model(local_vis, global_vis, physio, mask)
            probs = torch.sigmoid(logits).cpu().numpy().flatten()

            # æŒ‰ç—…äººèšåˆ
            for i, subj_id in enumerate(subject_ids):
                if subj_id not in patient_scores:
                    patient_scores[subj_id] = []
                patient_scores[subj_id].append(probs[i])

    # 4. ç”Ÿæˆç»“æœæ–‡ä»¶
    print(f"\nğŸ’¾ Generating {output_file} ...")

    # è·å–æ’åºåçš„ Subject ID åˆ—è¡¨ (ç¡®ä¿è¾“å‡ºé¡ºåºæ•´æ´)
    sorted_subjects = sorted(patient_scores.keys())

    lines_written = 0
    with open(output_file, 'w', encoding='utf-8') as f:
        for subj_id in sorted_subjects:
            scores = patient_scores[subj_id]

            # è®¡ç®—å¹³å‡æ¦‚ç‡ (Mean Voting)
            avg_prob = np.mean(scores)

            # äºŒå€¼åŒ–
            label = 1 if avg_prob > args.threshold else 0

            # æ ¼å¼: Test_028,0.798709,1
            line = f"{subj_id},{avg_prob:.6f},{label}"
            f.write(line + '\n')
            lines_written += 1

    print(f"âœ… Done! Written {lines_written} subjects to {output_file}.")
    print(f"   (Threshold used: {args.threshold})")


if __name__ == "__main__":
    main()