import os
import torch
import torch.optim as optim
import numpy as np
import wandb
from tqdm import tqdm
import sys
import argparse
import torch.nn as nn
# å¯¼å…¥é¡¹ç›®æ¨¡å—
import config
from models.sstnet import SSTNet
from utils.dataloader import get_loader
from utils.loss import get_loss_function
from utils.metrics import calculate_metrics
from utils.misc import fix_seed, save_checkpoint
from utils.loss import LabelSmoothingBCEWithLogitsLoss,SupConLoss


# train.py

def train_one_epoch(model, loader, criterion, optimizer, device, epoch):
    """
    è®­ç»ƒä¸€ä¸ª Epoch (v4.0: SupCon + Mixup + GNN Mask)
    """
    model.train()
    running_loss = 0.0

    # å®ä¾‹åŒ–å¯¹æ¯” Loss
    criterion_supcon = SupConLoss(temperature=0.07).to(device)
    # å¹³è¡¡ç³»æ•°
    lambda_supcon = 0.5
    # Mixup å‚æ•°
    mixup_alpha = 0.4
    # Mixup è§¦å‘æ¦‚ç‡ (å¯ä»¥è®¾ä¸º 1.0 å…¨ç¨‹å¼€å¯ï¼Œæˆ–è€… 0.5)
    mixup_prob = 1.0

    pbar = tqdm(loader, desc=f"Epoch {epoch + 1}/{config.EPOCHS} [Train]")

    for step, (local_vis, global_vis, physio, mask, label, subject_ids) in enumerate(pbar):
        # 1. æ•°æ®æ¬è¿
        local_vis = local_vis.to(device)
        global_vis = global_vis.to(device).squeeze(1)
        physio = physio.to(device)
        mask = mask.to(device)
        label = label.to(device)

        optimizer.zero_grad()

        # ================= [æ–°å¢] Mixup é€»è¾‘ =================
        use_mixup = (np.random.rand() < mixup_prob) and (local_vis.size(0) > 1)

        if use_mixup:
            # 1. ç”Ÿæˆ Mixup ç³»æ•° (Betaåˆ†å¸ƒ)
            lam = np.random.beta(mixup_alpha, mixup_alpha)

            # 2. ç”Ÿæˆä¹±åºç´¢å¼•
            index = torch.randperm(local_vis.size(0)).to(device)

            # 3. æ··åˆè¾“å…¥æ•°æ®
            # æ³¨æ„ï¼šmask é€šå¸¸å– max æˆ–è€…åŸå§‹ maskï¼Œè¿™é‡Œä¸ºäº†ç®€å•ç›´æ¥ç”¨åŸå§‹ mask (å‡è®¾é•¿åº¦åˆ†å¸ƒç±»ä¼¼)
            mixed_local = lam * local_vis + (1 - lam) * local_vis[index, :]
            mixed_global = lam * global_vis + (1 - lam) * global_vis[index, :]
            mixed_physio = lam * physio + (1 - lam) * physio[index, :]

            # 4. å‡†å¤‡æ··åˆæ ‡ç­¾ (ç”¨äº BCE Loss)
            label_a, label_b = label, label[index]

            # 5. å‰å‘ä¼ æ’­ (ä½¿ç”¨æ··åˆåçš„æ•°æ®)
            # æ³¨æ„ï¼šmask ä¸æ··åˆï¼Œç›´æ¥ç”¨å½“å‰çš„ mask
            outputs, proj_feats = model(mixed_local, mixed_global, mixed_physio, mask, return_proj=True)

            # 6. è®¡ç®— Mixup Loss
            # åˆ†ç±» Loss: æ··åˆç›®æ ‡çš„åŠ æƒå’Œ
            loss_cls = lam * criterion(outputs, label_a.float().unsqueeze(1)) + \
                       (1 - lam) * criterion(outputs, label_b.float().unsqueeze(1))

            # å¯¹æ¯” Loss (SupCon):
            # ç­–ç•¥: åªè®¡ç®—ä¸»æ ·æœ¬ (Label A) çš„ SupCon Loss
            # å› ä¸º mixed_feat ä¸»è¦è¿˜æ˜¯ä¿ç•™äº† Sample A çš„ç‰¹å¾ç»“æ„ (åœ¨ lam>0.5 æ—¶)
            loss_con = criterion_supcon(proj_feats, label_a)

        else:
            # === åŸå§‹é€»è¾‘ (ä¸ä½¿ç”¨ Mixup) ===
            outputs, proj_feats = model(local_vis, global_vis, physio, mask, return_proj=True)
            loss_cls = criterion(outputs, label.float().unsqueeze(1))
            loss_con = criterion_supcon(proj_feats, label)

        # ====================================================

        # æ€» Loss
        loss = loss_cls + lambda_supcon * loss_con

        loss.backward()
        optimizer.step()

        # è®°å½•æ•°æ®
        loss_val = loss.item()
        running_loss += loss_val

        # WandB è®°å½•
        wandb.log({"train_batch_loss": loss_val})
        pbar.set_postfix({"loss": f"{loss_val:.4f}"})

    return running_loss / len(loader)


def validate(model, loader, criterion, device):
    """
    éªŒè¯æ¨¡å‹ (ç—…äººçº§èšåˆè¯„ä¼°) - é›†æˆ TTA (æµ‹è¯•æ—¶å¢å¼º)
    """
    model.eval()
    running_loss = 0.0

    # ç»“æœå®¹å™¨: æŒ‰ subject_id èšåˆ
    patient_results = {}

    with torch.no_grad():
        # [ä¿®æ”¹] è§£åŒ… 6 ä¸ªå˜é‡
        for local_vis, global_vis, physio, mask, label, subject_ids in tqdm(loader, desc="[Val]"):
            local_vis = local_vis.to(device)
            global_vis = global_vis.to(device).squeeze(1)  # [B, 512]
            physio = physio.to(device)
            mask = mask.to(device)
            label = label.to(device)

            # ================= [æ–°å¢] TTA: é¢„æµ‹ 3 æ¬¡å–å¹³å‡ =================
            # 1. åŸå§‹é¢„æµ‹
            logits_1 = model(local_vis, global_vis, physio, mask)

            # 2. å¢å¼ºé¢„æµ‹ A (åŠ å¾®å¼±é«˜æ–¯å™ªéŸ³, æ¨¡æ‹Ÿä¼ æ„Ÿå™¨è¯¯å·®)
            noise_a = torch.randn_like(local_vis) * 0.01
            logits_2 = model(local_vis + noise_a, global_vis, physio, mask)

            # 3. å¢å¼ºé¢„æµ‹ B (ç¨å¾®å¤§ä¸€ç‚¹çš„å™ªéŸ³)
            noise_b = torch.randn_like(local_vis) * 0.02
            logits_3 = model(local_vis + noise_b, global_vis, physio, mask)

            # 4. å–å¹³å‡ (Logits å¹³å‡æ¯” Prob å¹³å‡æ›´ç¨³å®š)
            avg_logits = (logits_1 + logits_2 + logits_3) / 3.0
            # ==============================================================

            # è®¡ç®— Loss (ä½¿ç”¨ TTA åçš„ logits)
            loss = criterion(avg_logits, label.float().unsqueeze(1))
            running_loss += loss.item()

            # è½¬æ¦‚ç‡
            probs = torch.sigmoid(avg_logits).cpu().numpy().flatten()
            labels_np = label.cpu().numpy()

            # èšåˆé€»è¾‘
            for i, subj_id in enumerate(subject_ids):
                if subj_id not in patient_results:
                    patient_results[subj_id] = {'probs': [], 'label': labels_np[i]}
                patient_results[subj_id]['probs'].append(probs[i])

    # --- Patient-Level Metrics ---
    final_probs = []
    final_labels = []

    for subj_id, data in patient_results.items():
        # ç­–ç•¥: å¹³å‡åˆ†æŠ•ç¥¨ (Mean Voting)
        avg_prob = np.mean(data['probs'])
        final_probs.append(avg_prob)
        final_labels.append(data['label'])

    metrics = calculate_metrics(final_labels, final_probs)
    metrics['loss'] = running_loss / len(loader)

    return metrics


def main():
    parser = argparse.ArgumentParser(description="SSTNet v2.0 Training")
    parser.add_argument('--fold', type=int, default=0, choices=[0, 1, 2, 3],
                        help='Cross-validation fold index')
    args = parser.parse_args()

    current_fold = args.fold
    print(f"\nğŸš€ Starting Training for Fold: {current_fold} (v2.0 Global Aware)")
    print("=" * 40)

    fix_seed(config.SEED)

    wandb.init(
        project="SSTNet-v2",
        name=f"Fold{current_fold}_GlobalAware",
        config={
            "fold": current_fold,
            "learning_rate": config.LEARNING_RATE,
            "batch_size": config.BATCH_SIZE,
            "epochs": config.EPOCHS,
            "model": "SSTNet v2.0"
        }
    )

    device = torch.device(config.DEVICE if torch.cuda.is_available() else "cpu")

    # åŠ è½½æ•°æ®
    train_loader = get_loader('train', fold_idx=current_fold)
    val_loader = get_loader('val', fold_idx=current_fold)

    # æ„å»ºæ¨¡å‹
    model = SSTNet().to(device)
    wandb.watch(model, log="gradients", log_freq=100)

    # ä¼˜åŒ–å™¨ & æŸå¤±
    optimizer = optim.AdamW(
        model.parameters(),
        lr=config.LEARNING_RATE,
        weight_decay=config.WEIGHT_DECAY
    )
    # criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1.0]).to(device), label_smoothing=0.1)
    # print("âœ… Advanced Setup: BCEWithLogitsLoss with label_smoothing=0.1 enabled.")
    # [ä¿®æ”¹å]
    # 1. å®šä¹‰æ­£æ ·æœ¬æƒé‡ (å¦‚æœæ‚¨çš„æ•°æ®æ­£è´Ÿå‡è¡¡ï¼Œè®¾ä¸º 1.0 å³å¯)
    pos_weight = torch.tensor([1.0]).to(device)
    # 2. å®ä¾‹åŒ–è‡ªå®šä¹‰çš„ Loss ç±»ï¼Œè®¾ç½®å¹³æ»‘å› å­ä¸º 0.1
    criterion = LabelSmoothingBCEWithLogitsLoss(smoothing=0.1, pos_weight=pos_weight)

    print("âœ… Custom LabelSmoothingBCEWithLogitsLoss (smoothing=0.1) enabled.")
    #
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=config.EPOCHS, eta_min=1e-6
    )

    best_auc = 0.0
    save_dir = os.path.join(config.PROJECT_ROOT, 'checkpoints')
    os.makedirs(save_dir, exist_ok=True)

    for epoch in range(config.EPOCHS):
        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch)
        val_metrics = validate(model, val_loader, criterion, device)

        scheduler.step()

        wandb.log({
            "epoch": epoch + 1,
            "train_loss": train_loss,
            "val_loss": val_metrics['loss'],
            "val_auc": val_metrics['auc'],
            "val_acc": val_metrics['acc'],
            "val_f1": val_metrics['f1'],
            "val_sens": val_metrics['sensitivity'],
            "val_spec": val_metrics['specificity'],
            "val_prec": val_metrics['precision']
        })

        # print(f"Epoch {epoch + 1}: Train Loss: {train_loss:.4f} | Val AUC: {val_metrics['auc']:.4f}")
        print(f"-" * 80)
        print(f"Epoch {epoch + 1}/{config.EPOCHS} | Train Loss: {train_loss:.4f}")
        print(f"Validation Metrics:")
        print(f"   Accuracy:    [{val_metrics['acc']:.4f}]")
        print(f"   Sensitivity: [{val_metrics['sensitivity']:.4f}]")
        print(f"   Specificity: [{val_metrics['specificity']:.4f}]")
        print(f"   AUC:         [{val_metrics['auc']:.4f}]")
        print(f"   Precision:   [{val_metrics['precision']:.4f}]")
        print(f"   F1-score:    [{val_metrics['f1']:.4f}]")
        print(f"-" * 80)

        if val_metrics['auc'] > best_auc:
            best_auc = val_metrics['auc']
            save_path = os.path.join(save_dir, f"best_model_fold{current_fold}.pth")
            save_checkpoint(model, optimizer, epoch, best_auc, save_path)
            print(f"ğŸ’¾ Best Model Saved! AUC: {best_auc:.4f}")

    print(f"ğŸ† Fold {current_fold} Finished. Best AUC: {best_auc:.4f}")
    wandb.finish()


if __name__ == "__main__":
    main()