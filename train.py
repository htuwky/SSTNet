import os
import torch
import torch.optim as optim
import numpy as np
import wandb
from tqdm import tqdm
import sys
import argparse
import torch.nn as nn
# å¯¼å…¥é¡¹ç›®æ¨¡å—
import config
from models.sstnet import SSTNet
from utils.dataloader import get_loader
from utils.loss import get_loss_function
from utils.metrics import calculate_metrics
from utils.misc import fix_seed, save_checkpoint
from utils.loss import LabelSmoothingBCEWithLogitsLoss,SupConLoss

def train_one_epoch(model, loader, criterion, optimizer, device, epoch):
    """
    è®­ç»ƒä¸€ä¸ª Epoch (v2.0 å…¨å±€æ„ŸçŸ¥ç‰ˆ)
    """
    model.train()
    running_loss = 0.0
    # [æ–°å¢] å®ä¾‹åŒ–å¯¹æ¯” Loss
    criterion_supcon = SupConLoss(temperature=0.07).to(device)

    # [æ–°å¢] å¹³è¡¡ç³»æ•° (åˆ†ç±»å  1.0, å¯¹æ¯”å  0.5)
    lambda_supcon = 0.5
    pbar = tqdm(loader, desc=f"Epoch {epoch + 1}/{config.EPOCHS} [Train]")

    # [ä¿®æ”¹] è§£åŒ… 6 ä¸ªå˜é‡: local, global, physio, mask, label, id
    for step, (local_vis, global_vis, physio, mask, label, subject_ids) in enumerate(pbar):
        # # --- æ–°å¢è°ƒè¯•ä»£ç  ---
        # if step == 0:
        #     print(f"\nğŸ” [Debug Check] Labels in this batch: {label.tolist()}")
        #     print(f"ğŸ” [Debug Check] Subject IDs: {subject_ids}")
        # # ------------------
        #è°ƒè¯•æˆåŠŸï¼Œåˆ æ‰æ³¨é‡Š
        # # 1. æ•°æ®æ¬è¿
        local_vis = local_vis.to(device)  # [B, 32, 512]

        # [å…³é”®] å»æ‰ä¸­é—´çš„ç»´åº¦: [B, 1, 512] -> [B, 512]
        global_vis = global_vis.to(device).squeeze(1)

        physio = physio.to(device)
        mask = mask.to(device)
        label = label.to(device)

        optimizer.zero_grad()

        # [ä¿®æ”¹] å‰å‘ä¼ æ’­ï¼Œè¦æ±‚è¿”å›æŠ•å½±ç‰¹å¾
        # outputs æ˜¯ logits, proj_feats æ˜¯å½’ä¸€åŒ–çš„ç‰¹å¾
        outputs, proj_feats = model(local_vis, global_vis, physio, mask, return_proj=True)

        # 1. è®¡ç®—åˆ†ç±» Loss (ä¸»ä»»åŠ¡)
        loss_cls = criterion(outputs, label.float().unsqueeze(1))

        # 2. è®¡ç®—å¯¹æ¯” Loss (è¾…åŠ©ä»»åŠ¡)
        # æ³¨æ„ï¼šSupCon éœ€è¦ label æ¥çŸ¥é“è°å’Œè°æ˜¯åŒç±»
        loss_con = criterion_supcon(proj_feats, label)

        # 3. æ€» Loss
        loss = loss_cls + lambda_supcon * loss_con

        loss.backward()
        optimizer.step()

        # 6. è®°å½•æ•°æ®
        loss_val = loss.item()
        running_loss += loss_val

        # WandB è®°å½•
        wandb.log({"train_batch_loss": loss_val})
        pbar.set_postfix({"loss": f"{loss_val:.4f}"})

    return running_loss / len(loader)


def validate(model, loader, criterion, device):
    """
    éªŒè¯æ¨¡å‹ (ç—…äººçº§èšåˆè¯„ä¼°)
    """
    model.eval()
    running_loss = 0.0

    # ç»“æœå®¹å™¨: æŒ‰ subject_id èšåˆ
    patient_results = {}

    with torch.no_grad():
        # [ä¿®æ”¹] è§£åŒ… 6 ä¸ªå˜é‡
        for local_vis, global_vis, physio, mask, label, subject_ids in tqdm(loader, desc="[Val]"):
            local_vis = local_vis.to(device)
            global_vis = global_vis.to(device).squeeze(1)  # [B, 512]
            physio = physio.to(device)
            mask = mask.to(device)
            label = label.to(device)

            # å‰å‘ä¼ æ’­
            logits = model(local_vis, global_vis, physio, mask)

            # è®¡ç®— Loss
            loss = criterion(logits, label.float().unsqueeze(1))
            running_loss += loss.item()

            # è½¬æ¦‚ç‡
            probs = torch.sigmoid(logits).cpu().numpy().flatten()
            labels_np = label.cpu().numpy()

            # èšåˆé€»è¾‘
            for i, subj_id in enumerate(subject_ids):
                if subj_id not in patient_results:
                    patient_results[subj_id] = {'probs': [], 'label': labels_np[i]}
                patient_results[subj_id]['probs'].append(probs[i])

    # --- Patient-Level Metrics ---
    final_probs = []
    final_labels = []

    for subj_id, data in patient_results.items():
        # ç­–ç•¥: å¹³å‡åˆ†æŠ•ç¥¨ (Mean Voting)
        avg_prob = np.mean(data['probs'])
        final_probs.append(avg_prob)
        final_labels.append(data['label'])

    metrics = calculate_metrics(final_labels, final_probs)
    metrics['loss'] = running_loss / len(loader)

    return metrics


def main():
    parser = argparse.ArgumentParser(description="SSTNet v2.0 Training")
    parser.add_argument('--fold', type=int, default=0, choices=[0, 1, 2, 3],
                        help='Cross-validation fold index')
    args = parser.parse_args()

    current_fold = args.fold
    print(f"\nğŸš€ Starting Training for Fold: {current_fold} (v2.0 Global Aware)")
    print("=" * 40)

    fix_seed(config.SEED)

    wandb.init(
        project="SSTNet-v2",
        name=f"Fold{current_fold}_GlobalAware",
        config={
            "fold": current_fold,
            "learning_rate": config.LEARNING_RATE,
            "batch_size": config.BATCH_SIZE,
            "epochs": config.EPOCHS,
            "model": "SSTNet v2.0"
        }
    )

    device = torch.device(config.DEVICE if torch.cuda.is_available() else "cpu")

    # åŠ è½½æ•°æ®
    train_loader = get_loader('train', fold_idx=current_fold)
    val_loader = get_loader('val', fold_idx=current_fold)

    # æ„å»ºæ¨¡å‹
    model = SSTNet().to(device)
    wandb.watch(model, log="gradients", log_freq=100)

    # ä¼˜åŒ–å™¨ & æŸå¤±
    optimizer = optim.AdamW(
        model.parameters(),
        lr=config.LEARNING_RATE,
        weight_decay=config.WEIGHT_DECAY
    )
    # criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1.0]).to(device), label_smoothing=0.1)
    # print("âœ… Advanced Setup: BCEWithLogitsLoss with label_smoothing=0.1 enabled.")
    # [ä¿®æ”¹å]
    # 1. å®šä¹‰æ­£æ ·æœ¬æƒé‡ (å¦‚æœæ‚¨çš„æ•°æ®æ­£è´Ÿå‡è¡¡ï¼Œè®¾ä¸º 1.0 å³å¯)
    pos_weight = torch.tensor([1.0]).to(device)
    # 2. å®ä¾‹åŒ–è‡ªå®šä¹‰çš„ Loss ç±»ï¼Œè®¾ç½®å¹³æ»‘å› å­ä¸º 0.1
    criterion = LabelSmoothingBCEWithLogitsLoss(smoothing=0.1, pos_weight=pos_weight)

    print("âœ… Custom LabelSmoothingBCEWithLogitsLoss (smoothing=0.1) enabled.")
    #
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=config.EPOCHS, eta_min=1e-6
    )

    best_auc = 0.0
    save_dir = os.path.join(config.PROJECT_ROOT, 'checkpoints')
    os.makedirs(save_dir, exist_ok=True)

    for epoch in range(config.EPOCHS):
        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch)
        val_metrics = validate(model, val_loader, criterion, device)

        scheduler.step()

        wandb.log({
            "epoch": epoch + 1,
            "train_loss": train_loss,
            "val_loss": val_metrics['loss'],
            "val_auc": val_metrics['auc'],
            "val_acc": val_metrics['acc'],
            "val_f1": val_metrics['f1'],
            "val_sens": val_metrics['sensitivity'],
            "val_spec": val_metrics['specificity'],
            "val_prec": val_metrics['precision']
        })

        # print(f"Epoch {epoch + 1}: Train Loss: {train_loss:.4f} | Val AUC: {val_metrics['auc']:.4f}")
        print(f"-" * 80)
        print(f"Epoch {epoch + 1}/{config.EPOCHS} | Train Loss: {train_loss:.4f}")
        print(f"Validation Metrics:")
        print(f"   Accuracy:    [{val_metrics['acc']:.4f}]")
        print(f"   Sensitivity: [{val_metrics['sensitivity']:.4f}]")
        print(f"   Specificity: [{val_metrics['specificity']:.4f}]")
        print(f"   AUC:         [{val_metrics['auc']:.4f}]")
        print(f"   Precision:   [{val_metrics['precision']:.4f}]")
        print(f"   F1-score:    [{val_metrics['f1']:.4f}]")
        print(f"-" * 80)

        if val_metrics['auc'] > best_auc:
            best_auc = val_metrics['auc']
            save_path = os.path.join(save_dir, f"best_model_fold{current_fold}.pth")
            save_checkpoint(model, optimizer, epoch, best_auc, save_path)
            print(f"ğŸ’¾ Best Model Saved! AUC: {best_auc:.4f}")

    print(f"ğŸ† Fold {current_fold} Finished. Best AUC: {best_auc:.4f}")
    wandb.finish()


if __name__ == "__main__":
    main()